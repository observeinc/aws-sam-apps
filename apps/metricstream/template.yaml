---
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31
Description: 'Stream CloudWatch Metrics to S3.'
Metadata:
  AWS::ServerlessRepo::Application:
    Name: observe-metric-stream
    Description: Stream CloudWatch Metrics to S3.
    Author: Observe Inc
    SpdxLicenseId: Apache-2.0
    ReadmeUrl: README.md
    HomePageUrl: https://github.com/observeinc/aws-sam-apps
    SemanticVersion: '0.0.1'
    SourceCodeUrl: https://github.com/observeinc/aws-sam-apps

Parameters:
  BucketArn:
    Type: String
    Description: >-
      S3 Bucket ARN to write metrics.
    AllowedPattern: "^arn:.*$"
  Prefix:
    Type: String
    Description: >-
      Optional prefix to write metrics to.
    Default: ''
  FilterUri:
    Type: String
    Description: >-
      A file hosted in S3 containing list of metrics to stream.
    Default: 's3://observeinc/cloudwatchmetrics/filters/empty.yaml'
<<<<<<< HEAD
    AllowedPattern: "^(s3:\/\/.*)?$"
=======
    AllowedPattern: "^s3:\/\/.*"
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
  OutputFormat:
    Type: String
    Description: >-
      The output format for CloudWatch Metrics.
    Default: 'json'
    AllowedValues:
      - json
      - opentelemetry0.7
      - opentelemetry1.0
  NameOverride:
    Type: String
    Description: >-
      Set Firehose Delivery Stream name. In the absence of a value, the stack
      name will be used.
    Default: ''
    MaxLength: 64
  BufferingInterval:
    Type: Number
    Default: 60
    MinValue: 60
    MaxValue: 900
    Description: |
      Buffer incoming data for the specified period of time, in seconds, before
      delivering it to the destination.
  BufferingSize:
    Type: Number
    Default: 1
    MinValue: 1
    MaxValue: 64
    Description: |
      Buffer incoming data to the specified size, in MiBs, before delivering it
      to the destination.
  ObserveAccountID:
    Type: String
    Description: Observe Account ID
    AllowedPattern: '\d*'
    Default: ''
  ObserveDomainName:
    Type: String
    Description: >-
      The domain name we will retrieve metric configuration from.
    Default: ''
  UpdateTimestamp:
    Type: String
    Description: Timestamp when the metric stream was created or updated.
    Default: ''
    AllowedPattern: '^[0-9]*$'
  DatasourceID:
    Type: String
    Description: >-
      The datasource for this metric stream.
    Default: ''
    AllowedPattern: '\d*'
  GQLToken:
    Type: String
    NoEcho: true
    Description: >-
      The token used to retrieve metric configuration.
    Default: ''
Conditions:
  UseStackName: !Equals
    - !Ref NameOverride
    - ''
<<<<<<< HEAD
  DeployLambda: !Not
    - !Equals
      - !Ref DatasourceID
      - ""
=======
  EmptyFilterUri: !Equals
    - !Ref FilterUri
    - ""
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)

Resources:
  DeliveryStreamRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - firehose.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: logging
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !GetAtt LogGroup.Arn
        - PolicyName: s3writer
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                Resource:
                  - !Ref BucketArn
                  - !Sub '${BucketArn}/${Prefix}*'
  LambdaLogGroup:
    Type: 'AWS::Logs::LogGroup'
    Properties:
      LogGroupName: !Join
        - ''
        - - /aws/lambda/
          - !If
            - UseStackName
            - !Ref AWS::StackName
            - !Ref NameOverride
<<<<<<< HEAD
          - '-'
          - !Select [2, !Split ["/", !Ref "AWS::StackId"]]
          # This grabs the unique id part of the stack id.
          # This part is unique every time the stack is created.
          # the reason we have this is to avoid an error
          # if the stack is destroyed and created again.
          # If the log group name is the same on recreation,
          # an error will be thrown.
=======
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
      RetentionInDays: 365
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
  LogGroup:
    Type: 'AWS::Logs::LogGroup'
    Properties:
      LogGroupName: !Join
        - ''
        - - /aws/firehose/
          - !If
            - UseStackName
            - !Ref AWS::StackName
            - !Ref NameOverride
      RetentionInDays: 365
  LogStream:
    Type: 'AWS::Logs::LogStream'
    Properties:
      LogStreamName: s3logs
      LogGroupName: !Ref LogGroup
  DeliveryStream:
    Type: 'AWS::KinesisFirehose::DeliveryStream'
    Properties:
      DeliveryStreamName: !If
        - UseStackName
        - !Ref AWS::StackName
        - !Ref NameOverride
      DeliveryStreamType: DirectPut
      S3DestinationConfiguration:
        BucketARN: !Ref BucketArn
        RoleARN: !GetAtt DeliveryStreamRole.Arn
        # yamllint disable-line rule:line-length
        Prefix: !Sub '${Prefix}AWSLogs/${AWS::AccountId}/cloudwatchmetrics/${AWS::Region}/${OutputFormat}/'
        # yamllint disable-line rule:line-length
        ErrorOutputPrefix: !Sub '${Prefix}AWSLogs/${AWS::AccountId}/cloudwatchmetrics/${AWS::Region}/errors/'
        BufferingHints:
          IntervalInSeconds: !Ref BufferingInterval
          SizeInMBs: !Ref BufferingSize
        CloudWatchLoggingOptions:
          Enabled: true
          LogGroupName: !Ref LogGroup
          LogStreamName: !Ref LogStream
  MetricStreamRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - streams.metrics.cloudwatch.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      Policies:
        - PolicyName: firehose
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - firehose:DescribeDeliveryStream
                  - firehose:ListDeliveryStreams
                  - firehose:ListTagsForDeliveryStream
                  - firehose:PutRecord
                  - firehose:PutRecordBatch
                Resource: !GetAtt 'DeliveryStream.Arn'
  MetricStream:
    Type: AWS::CloudWatch::MetricStream
    Metadata:
      cfn-lint:
        config:
          ignore_checks:
            # Disable false positive when using Fn::Transform
            # https://github.com/aws-cloudformation/cfn-lint/issues/2887
            - E3002
    Properties:
      FirehoseArn: !GetAtt 'DeliveryStream.Arn'
      RoleArn: !GetAtt MetricStreamRole.Arn
      'Fn::Transform':
        Name: "AWS::Include"
        Parameters:
          Location: !Ref FilterUri
      # End of processable content for include.
      # AWS::Include will be replaced with the file FilterUri references.
      # This macro always runs, so we must always provide it with a valid
      # S3 file, even if we overwrite it with a lambda.
      # filterURI is decided in the parent stack becauseAWS does not
      # allow an !If function inside an AWS::Include.
      OutputFormat: !Ref OutputFormat
<<<<<<< HEAD
  MetricsConfiguratorRole:
    Type: AWS::IAM::Role
    Condition: DeployLambda
=======
  MetricsRecorderExecutionRole:
    Type: AWS::IAM::Role
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
<<<<<<< HEAD
        - PolicyName: MetricsConfiguratorPolicy
=======
        - PolicyName: S3andMetricStreamPutPolicy
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cloudwatch:PutMetricStream
                Resource: !GetAtt MetricStream.Arn
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource: !GetAtt MetricStreamRole.Arn
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !GetAtt LambdaLogGroup.Arn
<<<<<<< HEAD
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                Resource: !Ref GQLTokenSecret
  MetricsConfigurator:
    Type: AWS::Serverless::Function
    Condition: DeployLambda
=======
  MetricsRecorder:
    Type: AWS::Serverless::Function
    Condition: EmptyFilterUri
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
    Metadata:
      BuildMethod: makefile
    Properties:
      FunctionName: !If
        - UseStackName
        - !Ref AWS::StackName
        - !Ref NameOverride
<<<<<<< HEAD
      Role: !GetAtt MetricsConfiguratorRole.Arn
=======
      Role: !GetAtt MetricsRecorderExecutionRole.Arn
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
      CodeUri: ../../bin/linux_arm64
      Handler: bootstrap
      Runtime: provided.al2
      Architectures:
        - arm64
<<<<<<< HEAD
      LoggingConfig:
        LogGroup: !Ref LambdaLogGroup
      Environment:
        Variables:
          VERBOSITY: 6
          ACCOUNT_ID: !Ref ObserveAccountID
          OBSERVE_DOMAIN_NAME: !Ref ObserveDomainName
          DATASOURCE_ID: !Ref DatasourceID
          SECRET_NAME: !Ref GQLTokenSecret
=======
      Environment:
        Variables:
          VERBOSITY: 6
          # fields needed for accessing gql
          BEARER_TOKEN: "rlA6DnekcdW8OYxyuoaBoRhWXkOec1tC"
          ACCOUNT_NUMBER: 124375634991
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
          # fields are necessary to update the metric stream
          METRIC_STREAM_NAME: !Ref MetricStream
          FIREHOSE_ARN: !GetAtt DeliveryStream.Arn
          ROLE_ARN: !GetAtt MetricStreamRole.Arn
          OUTPUT_FORMAT: !Ref OutputFormat
<<<<<<< HEAD
  GQLTokenSecret:
    Type: AWS::SecretsManager::Secret
    Condition: DeployLambda
    Properties:
      Description: GQL Token Secret
      Name: !Sub "observe-gql-token-${AWS::StackName}"
      SecretString: !Ref GQLToken
  StackCreationUpdateCustomResource:
    Type: Custom::StackCreationUpdateTrigger
    Condition: DeployLambda
    Properties:
      ServiceToken: !GetAtt MetricsConfigurator.Arn
      StackName: !Ref AWS::StackName
      UpdateTimestamp: !Ref UpdateTimestamp
=======
  StackCreationUpdateCustomResource:
    Type: Custom::StackCreationUpdateTrigger
    Condition: EmptyFilterUri
    Properties:
      ServiceToken: !GetAtt MetricsRecorder.Arn
      StackName: !Ref AWS::StackName
      AccessToken: "rlA6DnekcdW8OYxyuoaBoRhWXkOec1tC"
>>>>>>> f12c866 (OB-36691 feat: add lambda and s3 bucket to record push metrics config)
Outputs:
  FirehoseArn:
    Description: >-
      Kinesis Firehose Delivery Stream ARN. CloudWatch Metric Streams
      subscribed to this Firehose will have their metrics batched and written
      to S3.
    Value: !GetAtt 'DeliveryStream.Arn'
  LogGroupName:
    Description: >-
      Firehose Log Group Name. This log group will contain debugging
      information if Firehose fails to deliver data to S3.
    Value: !Ref LogGroup
